{"componentChunkName":"component---src-templates-blog-post-tsx","path":"/blog/deepseek-and-ollama-running-llms-locally-with-the-power-of-your-gpu/","result":{"data":{"markdownRemark":{"frontmatter":{"title":"DeepSeek and Ollama: Running LLMs locally with the power of your GPU","date":"January 29, 2025","author":"David Costa","cover":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/2bd797bfc000f3138130b63a2cf0a16c/86f91/cover.webp","srcSet":"/static/2bd797bfc000f3138130b63a2cf0a16c/bf18b/cover.webp 200w,\n/static/2bd797bfc000f3138130b63a2cf0a16c/d23e1/cover.webp 400w,\n/static/2bd797bfc000f3138130b63a2cf0a16c/86f91/cover.webp 800w","sizes":"(min-width: 800px) 800px, 100vw"},"sources":[]},"width":800,"height":483}}}},"html":"<p>The AI and LLMs development universe is shifting rapidly. Just last week, Nvidia, the powerhouse behind AI hardware saw its stock going down, Why you ask? Because the world is realizing that you don’t need to rely on massive cloud infrastructure or expensive GPUs to harness the power of AI.</p>\n<p>Deepseek, the new model deveolped by the chinese DeepSeek\nInc offered an alternative to models already consolidated in the market.</p>\n<p>But let's forget the volatility of the stock market and learn how we can run a local instance of Deepseek in our machines step by step.</p>\n<br/>\n<hr>\n<br/>\n<h2>Step 1: Install Ollama</h2>\n<br/>\n<p>First, you will need to install <a href=\"https://ollama.com/download\">Ollama</a> on your machine. Open your terminal and run the following command:</p>\n<br/>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\">$ <span class=\"token function\">curl</span> <span class=\"token parameter variable\">-fsSL</span> https://ollama.com/install.sh <span class=\"token operator\">|</span> <span class=\"token function\">sh</span></code></pre></div>\n<br/>\n<p>This script will set up Ollama on your system. But what is Ollama David?</p>\n<p>Basically, Ollama is an open-source tool that runs large language models (LLMs) directly on a local machine using the power of your GPU.</p>\n<br/>\n<hr>\n<br/>\n<h2>Step 2: Verify the Installation</h2>\n<p>Once installed, check if Ollama is working by running:</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\">$ ollama</code></pre></div>\n<p>If you see a response, ollama was installed successfully.</p>\n<br/>\n<hr>\n<br/>\n<h2>Step 3: Choose Your DeepSeek Model</h2>\n<br/>\n<p>Ollama supports multiple versions of DeepSeek. The two main models available are:</p>\n<ol>\n<li><a href=\"https://ollama.com/library/deepseek-r1\">DeepSeek-R1</a></li>\n</ol>\n<ul>\n<li>Parameters: 7B (7 billion parameters).</li>\n<li>Best for: Beginners, lightweight tasks, and users with limited hardware.</li>\n<li>Use cases: Simple text generation, basic coding assistance, and experimentation.</li>\n<li>Hardware requirements: Can run on CPUs or low-end GPUs.</li>\n</ul>\n<ol start=\"2\">\n<li><a href=\"https://ollama.com/library/deepseek-v3\">DeepSeek-V3</a></li>\n</ol>\n<ul>\n<li>Parameters: 70B (70 billion parameters).</li>\n<li>Best for: Advanced users, complex tasks, and professional use cases.</li>\n<li>Use cases: Advanced coding, research, creative writing, and large-scale data analysis.</li>\n<li>Hardware requirements: Requires a high-end GPU for optimal performance.</li>\n</ul>\n<br/>\n<h3>What Are Parameters?</h3>\n<br/>\n<p>Parameters are the building blocks of AI models. They determine how much knowledge and complexity the model can handle. Here’s a quick breakdown:</p>\n<ul>\n<li>\n<p>7B (7 billion parameters): Smaller models like DeepSeek-R1 are faster and require less computational power. They’re great for basic tasks and experimentation.</p>\n</li>\n<li>\n<p>70B (70 billion parameters): Larger models like DeepSeek-V3 are more powerful and capable of handling complex tasks with higher accuracy. However, they require more resources to run.</p>\n</li>\n</ul>\n<br/>\n<hr>\n<br/>\n<h2>Step 4: Run Your Chosen Model</h2>\n<br/>\n<p>Once you’ve decided on a model, you can run it using Ollama. For example:</p>\n<br/>\n<ul>\n<li>To run DeepSeek-R1:</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\">$ ollama run deepseek-r1</code></pre></div>\n<br/>\n<ul>\n<li>To run DeepSeek-V3:</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\">$ ollama run deepseek-v3</code></pre></div>\n<br/>\n<hr>\n<br/>\n<h2>Step 5: Start Experimenting!</h2>\n<br/>\n<p>Now that your model is running, you can use it for:</p>\n<br/>\n<ul>\n<li>Coding assistance: Generate code snippets or debug errors.</li>\n<li>Creative writing: Write stories, poems, or scripts.</li>\n<li>Research: Analyze data or generate insights.</li>\n<li>Experiment with parameters: Adjust the model’s settings to optimize performance for your specific use case. Ollama has a lot of models to explore.</li>\n</ul>\n<br/>\n<hr>\n<br/>\n<h2>Conclusion</h2>\n<br/>\n<p>Whether you’re a beginner or an advanced user, running DeepSeek locally with Ollama opens up a world of possibilities. Start with DeepSeek-R1 for lightweight tasks or go all-in with DeepSeek-V3 for advanced projects.</p>"}},"pageContext":{"slug":"deepseek-and-ollama-running-llms-locally-with-the-power-of-your-gpu"}},"staticQueryHashes":[],"slicesMap":{}}